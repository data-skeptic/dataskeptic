{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Today's mini episode discusses the widely known optimization algorithm gradient descent in the context of hiking in a foggy hillside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.bogotobogo.com/python/python_numpy_batch_gradient_descent_algorithm.php\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "import pylab\n",
    "from scipy import stats\n",
    "\n",
    "def gradient_descent(alpha, x, y, ep=0.0001, max_iter=10000):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0] # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    # total error, J(theta)\n",
    "    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "    \n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # mean squared error\n",
    "        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "\n",
    "        if abs(J-e) <= ep:\n",
    "            print 'Converged, iterations: ', iter, '!!!'\n",
    "            converged = True\n",
    "    \n",
    "        J = e   # update error \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print 'Max interactions exceeded!'\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "    print 'x.shape = %s y.shape = %s' %(x.shape, y.shape)\n",
    " \n",
    "    alpha = 0.01 # learning rate\n",
    "    ep = 0.01 # convergence criteria\n",
    "\n",
    "    # call gredient decent, and get intercept(=theta0) and slope(=theta1)\n",
    "    theta0, theta1 = gradient_descent(alpha, x, y, ep, max_iter=1000)\n",
    "    print ('theta0 = %s theta1 = %s') %(theta0, theta1) \n",
    "\n",
    "    # check with scipy linear regression \n",
    "    slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "    print ('intercept = %s slope = %s') %(intercept, slope) \n",
    "\n",
    "    # plot\n",
    "    for i in range(x.shape[0]):\n",
    "        y_predict = theta0 + theta1*x \n",
    "\n",
    "    pylab.plot(x,y,'o')\n",
    "    pylab.plot(x,y_predict,'k-')\n",
    "    pylab.show()\n",
    "    print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "import pylab\n",
    "from scipy import stats\n",
    "\n",
    "def gradient_descent_2(alpha, x, y, numIterations):\n",
    "    m = x.shape[0] # number of samples\n",
    "    theta = np.ones(2)\n",
    "    x_transpose = x.transpose()\n",
    "    for iter in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        J = np.sum(loss ** 2) / (2 * m)  # cost\n",
    "        print \"iter %s | J: %.3f\" % (iter, J)      \n",
    "        gradient = np.dot(x_transpose, loss) / m         \n",
    "        theta = theta - alpha * gradient  # update\n",
    "    return theta\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "    m, n = np.shape(x)\n",
    "    x = np.c_[ np.ones(m), x] # insert column\n",
    "    alpha = 0.01 # learning rate\n",
    "    theta = gradient_descent_2(alpha, x, y, 1000)\n",
    "\n",
    "    # plot\n",
    "    for i in range(x.shape[1]):\n",
    "        y_predict = theta[0] + theta[1]*x \n",
    "    pylab.plot(x[:,1],y,'o')\n",
    "    pylab.plot(x,y_predict,'k-')\n",
    "    pylab.show()\n",
    "    print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# m denotes the number of examples here, not the number of features\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "\n",
    "def genData(numPoints, bias, variance):\n",
    "    x = np.zeros(shape=(numPoints, 2))\n",
    "    y = np.zeros(shape=numPoints)\n",
    "    # basically a straight line\n",
    "    for i in range(0, numPoints):\n",
    "        # bias feature\n",
    "        x[i][0] = 1\n",
    "        x[i][1] = i\n",
    "        # our target variable\n",
    "        y[i] = (i + bias) + random.uniform(0, 1) * variance\n",
    "    return x, y\n",
    "\n",
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "x, y = genData(100, 25, 10)\n",
    "m, n = np.shape(x)\n",
    "numIterations= 100000\n",
    "alpha = 0.0005\n",
    "theta = np.ones(n)\n",
    "theta = gradientDescent(x, y, theta, alpha, m, numIterations)\n",
    "print(theta)"
   ]
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
