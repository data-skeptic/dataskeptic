<? include("../header.php") ?>

<div id="bbody">
<h1>The t-test</h1>

<p>The t-test is this week's mini-episode topic.  The t-test is a statistical
testing procedure used to determine if the mean of two datasets differs by a
statistically significant amount.  We discuss how a wine manufacturer might
apply a t-test to determine if the sweetness, acidity, or some other property
of two separate grape vines might differ in a statistically meaningful way.</p>

<p>While human height is often given as a great example of when to use a
t-test - comparing the mean height of women to the mean height of men - this
is actually not necessarily the right test given that
<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2831262/">human height 
is not normally distributioned</a>, which is a
requirement for the t-test to be applicable.</p>

<h2>Example of the t-test</h2>

<p>How to test to see if your data is normally distributioned is a topic
for another day, but 
<a href="http://allendowney.blogspot.com/2013/08/are-my-data-normal.html">
Allen Downey's "Are My Data Normal"</a> is a great blog post on this.</p>

<p>Let's use the sample data below and walk through how to conduct this test in Excel and in R.

<p><b>Group 1:</b> 40, 47, 55, 41, 47, 36, 34, 50, 67, 46, 42, 31, 50, 43, 53, 38, 43, 76, 39, 55, 53, 42, 46, 58, 55, 59, 67, 35, 55, 24, 51, 50, 62, 55, 63, 54, 51, 50, 43, 53, 55, 54,
51, 66, 44, 57, 55, 51, 54, 46, 42, 41, 59, 43, 65, 54, 56, 77, 50, 45, 58, 38, 63, 53, 56, 61, 62, 47, 42, 54, 51, 62, 34, 54, 40, 47, 37, 37, 53, 45, 43, 45, 54, 46,
38, 69, 54, 35, 55, 39, 58, 53, 57, 49, 42, 44, 57, 58, 45, 43</p>

<p><b>Group 2:</b> 68, 67, 57, 61, 50, 62, 69, 56, 61, 39, 65, 71, 61, 46, 67, 52, 69, 54, 64, 55, 54, 52, 59, 72, 64, 65, 58, 66, 56, 64, 61, 44, 56, 70, 51, 56, 46, 54, 28, 60, 60, 76,
80, 43, 51, 68, 55, 58, 66, 47, 53, 58, 61, 63, 45, 55, 68, 47, 53, 63, 65, 61, 56, 57, 41, 58, 45, 58, 58, 46, 71, 50, 57, 56, 59, 67, 74, 52, 61, 67, 85, 58, 59, 50,
52, 51, 67, 54, 80, 72, 61, 53, 69, 57, 63, 49, 64, 57, 66, 59</p>

<p>Below are histograms for each of the groups, followed by a line plot comparing them.  These plots should convince you these populations do not have equal means.  While a
visual inspection is possible in this case, a more rigorous approach is to use the t-test to determine if these populations have different means.</p>

<!--begin.rcode echo=FALSE
x1 = c(40, 47, 55, 41, 47, 36, 34, 50, 67, 46, 42, 31, 50, 43, 53, 38, 43, 76, 39, 55, 53, 42, 46, 58, 55, 59, 67, 35, 55, 24, 51, 50, 62, 55, 63, 54, 51, 50, 43, 53, 55, 54,
51, 66, 44, 57, 55, 51, 54, 46, 42, 41, 59, 43, 65, 54, 56, 77, 50, 45, 58, 38, 63, 53, 56, 61, 62, 47, 42, 54, 51, 62, 34, 54, 40, 47, 37, 37, 53, 45, 43, 45, 54, 46,
38, 69, 54, 35, 55, 39, 58, 53, 57, 49, 42, 44, 57, 58, 45, 43)

x2 = c(68, 67, 57, 61, 50, 62, 69, 56, 61, 39, 65, 71, 61, 46, 67, 52, 69, 54, 64, 55, 54, 52, 59, 72, 64, 65, 58, 66, 56, 64, 61, 44, 56, 70, 51, 56, 46, 54, 28, 60, 60, 76,
80, 43, 51, 68, 55, 58, 66, 47, 53, 58, 61, 63, 45, 55, 68, 47, 53, 63, 65, 61, 56, 57, 41, 58, 45, 58, 58, 46, 71, 50, 57, 56, 59, 67, 74, 52, 61, 67, 85, 58, 59, 50,
52, 51, 67, 54, 80, 72, 61, 53, 69, 57, 63, 49, 64, 57, 66, 59)

breaks = seq(0,100,by=10)
h1 = hist(x1, breaks=breaks, main="Histogram of Group 1")
h2 = hist(x2, breaks=breaks, main="Histogram of Group 2")
b = breaks[1:(length(breaks)-1)]
ymax = max(h1$counts,h2$counts)
plot(b, h1$counts, type='l', col="blue", axes=FALSE, ylim=c(0, ymax))
points(b, h2$counts, type='l', col="green")
axis(1)
abline(v=mean(x1), col="blue", lwd=2)
abline(v=mean(x2), col="green", lwd=2)
y = ymax/2
text(mean(x1), y, "Group 1 mean", srt=90, pos=2)
text(mean(x2), y, "Group 2 mean", srt=90, pos=2)
end.rcode-->

<h2>How to run a t-test in Excel</h2>
<p>Assuming you put this data into Excel in columns A and B starting in row 2, this excel command will calculate the p-value:</p>

<pre>
=T.TEST(A2:A101,B2:B101,2,2)
</pre>

<h2>How to run a t-test in R</h2>
<p>The code below demonstrates the execution of a t-test in R.</p>

<!--begin.rcode
  x1 = c(55, 56, 57, 60, 46, 59, 49, 61, 58, 57, 71, 44, 51, 46, 42, 49, 50, 49, 38, 59)
  x2 = c(64, 45, 57, 56, 62, 83, 65, 50, 73, 66, 64, 72, 45, 42, 66, 48, 60, 61, 62, 69)
  eval = t.test(x1, x2, paired=FALSE)
  eval
  eval$p.value
end.rcode-->

<h2>Skepticism of the t-test explored through simulation</h2>
<p>When I was an undergraduate, I developed a practice that helped me a great deal in understanding statistical methods.  I frequently
generated small datasets of random numbers, and tested them.  Any statistical test worth consideration should accept the null hypothesis
the majority of the time when tested on random data.  Next, I would introduce a small but deliberate bias to the random data and see
how the results change.  How small did perturbations have to be to go unnoticed?  How quickly would large perturbations be highlighted?</p>

<p>These exercises were emensely helpful to my development, so let's do one together.  Below is some R code that generates sets of 
normally distributed random numbers and runs a t-test on the data.  Since I am providing random data, I expect the t-test to not find
any difference in means most of the time.  The plot which follows the code shows the p-values found by the t-tests.</p>

<!--begin.rcode
pvals= c()
sampleSizes = floor(seq(10, 100000, length=400))
for (n in sampleSizes) {
  x1 = rnorm(n) # random population 1
  x2 = rnorm(n) # random population 2
  eval = t.test(x1, x2, paired=FALSE)
  pval = eval$p.value
  pvals = c(pvals, pval)
}
falsePositives = sum(pvals < 0.05) / length(pvals)
hist(pvals, breaks=seq(0,1,length=20))
abline(v=0.05, lwd=2, col="blue")
text(0.05, 25, expression(alpha), pos=4)
text(0.05, 25, "   = 0.05", pos=4)
end.rcode-->

<p>The code above iteratively calculates randomly data and then performs a t-test, and captures the p-value.  The plot is a histogram 
which summaries how often each range of p-values is observed in these trials.  Remembering that a p-value is the probability that any difference in the populations is due to chance.  Thus, we expect a wide range of p-values since we know the data is in fact random, and this is what we find.</p>

<p>Further, assuming $\alpha = 0.05$, notice that <!--rinline round(100*falsePositives,2)-->% of trials report the difference in populations to be statistically significant, which we know is false!  This is not a flaw in the test, but a useful observation.  The t-test will occasionally find a statistically significant difference in data which (unknown to the tester) has no real difference.  In this case, since we created our data, we know it should be random.  However, in general, we have no access to ground truth - we do tests to determine if changes are meaningful.  One should always keep in mind that the choosen $\alpha$ value defines the frequency with which an incorrect result will be generated.</p>

<h2>How sensitive is the t-test?</h2>
<p>So how sensitive is the t-test to small changes?  Let's run another simulation in which we add a very small difference to our two populations, and measure how big a sample size is required to detect it.  In the next set of simulations, I'll make the second population 0.02 higher than the first population.  The plot below how many iterations it took before a statistically significant example was detected (giving up after 2000 iterations)</p>

<!--begin.rcode echo=FALSE
numIterations = 2000
trials = 200
alpha = 0.05
sampleSizes = floor(seq(10, 100000, length=trials))
iters=rep(NA, length(sampleSizes))
i=1
for (n in sampleSizes) {
  pval = 0
  c = 0
  while (pval < alpha && c < numIterations) {
    x1 = rnorm(n) # random population 1
    x2 = rnorm(n) + 0.02 # random population 2
    eval = t.test(x1, x2, paired=FALSE)
    pval = eval$p.value
    c=c+1
    if (pval < alpha) {
      iters[i] = c
    }
  }
  i = i + 1
}
ym = max(iters[!is.na(iters)])*1.1
plot(sampleSizes, iters, xlab="Sample Size", ylab="iterations before detection", axes=FALSE, cex=0.5, ylim=c(0, ym))
xat=seq(0, max(sampleSizes), by=10000)
axis(1, at=xat, labels=xat)
axis(2)
end.rcode-->

<p>This is intutive, in that a small variation is shown to be undetectable at small sample sizes, and increasingly more detectable with larger sample sizes</p>

<h2>Problems with the t-test in practice</h2>
<p>In practice, I have consistently found that by increasing my dataset to a large enough size, I can <u>always</u> generate a result
in which I reject the null hypothesis; in other words, more data can always generate a statistically significant result.  I was not able
to replicate that using randomly generated data - which is good.  I suppose this highlights the difference between simulated data and real world data.  In a future blog post, I'll explore this situation using some real data sets to demonstrate it.</p>

<p>Last but not least, we failed to discuss in this episode of the podcast how important it is for one to consider effect size of
the result.  Statistical significance testing helps you determine if an observed difference in two populations is due to chance
or not, but it tells you nothing about if the significant difference is large enough to be of consequence to you.</p>

 <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script type="text/javascript">MathJax.Hub.Config({tex2jax: {processEscapes: true,
        processEnvironments: false, inlineMath: [ ['$','$'] ],
        displayMath: [ ['$$','$$'] ] },
        asciimath2jax: {delimiters: [ ['$','$'] ] },
        "HTML-CSS": {minScaleAdjust: 125 } });
    </script>

<? include("../footer.php") ?>

