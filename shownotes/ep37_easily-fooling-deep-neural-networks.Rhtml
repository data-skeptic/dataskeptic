<? include("../header.php") ?>

<div id="bbody">
<h1>Easily Fooling Deep Neural Networks</h1>

<p>My guest this week is Anh Nguyen, a PhD student at the University of Wyoming working in
the <a href="http://www.evolvingai.org/">Evolving AI lab</a>.
The episode discusses the paper
<a href="http://arxiv.org/pdf/1412.1897v2.pdf">Deep Neural Networks are Easily Fooled [pdf]</a>
by Anh Nguyen, Jason Yosinski, and Jeff Clune with guest Anh Nguyen.  It describes
work creating images which a deep neural network (which has been trained to recognize
certain objects) will mis-identify (or be fooled) by images that to a human observer are clearly
not images of the given category choosen by the network.  Previous work had shown that
some images which appear to be unrecognizable white noise images to us can fool a deep neural network.
This paper extends the result showing abstract images of shapes and colors, many of which have form
(just not the one the network thinks) can also trick the network.</p>

<p>The core of the confusion, as Anh shares, seems to be in the 
discriminative nature of these networks.  Their objective is to find any features which allow
them to objectively distinguish between their available training image.  This creates the opportunity
for a type of over-fitting (or perhaps one could argue in some cases it's <i>under</i> fitting) 
yielding this situation.</p>

<p>We discuss the paper and it's implications including how this might effect security system
uses of neural networks or even self driving cars.</p>

<p>I highly recommend checking out their original paper (pdf link above) for examples of the
images.  There is also a great youtube video <a href="https://www.youtube.com/watch?v=M2IebCN9Ht4">Deep Neural Networks are Easily Fooled</a>
that explains the phenomenon in a clear and visual way.</p>

<p>For his benevolent recommendation, Anh suggests listeners might find the 
<a href="http://caffe.berkeleyvision.org/">Caffe image recognition software package</a>.
And please check out work by Anh and his collaborators at the 
<a href="http://www.evolvingai.org/">Evolving Artificial Intelligence Laboratory</a>.

<? include("../footer.php") ?>
