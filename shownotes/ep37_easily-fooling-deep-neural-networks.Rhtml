<? include("../header.php") ?>

<div id="bbody">
<h1>Easily Fooling Deep Neural Networks</h1>

<p>
</p>

<p>The episode features a discussion of the recent paper
<a href="http://arxiv.org/pdf/1412.1897v2.pdf">Deep Neural Networks are Easily Fooled [pdf]</a>
by Anh Nguyen, Jason Yosinski, and Jeff Clune with guest Anh Nguyen.  This paper describes
work creating images which are poorly classified by a trained deep neural network.  The
researchers showed that is is possible to produce both unrecognizable hite noise images, as well
as abstract images of shapes and colors, and evolve them to a state of high classification
confidence into classes they don't belong in.</p>

<p>We discuss the paper and it's implications including how this might effect security system
uses of neural networks or even self driving cars.</p>

<p>I highly recommend checking out their original paper (pdf link above) for examples of the
images.  There is also a great youtube video <a href="https://www.youtube.com/watch?v=M2IebCN9Ht4">Deep Neural Networks are Easily Fooled</a>
that explains the phenomenon in a clear and visual way.</p>

<? include("../footer.php") ?>
